{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Logistic Regression Model for Titanic Problem\n",
    "\n",
    "We will first use six input features that make sense to contribute to the survival of a passenger; they are: pclass, sex, age, sibsp, parch, cabin.\n",
    "The other features - fare, name and ticket number - seem to have little effect on a passenger's survival.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of this project\n",
    "\n",
    "We can break the code into few parts\n",
    "*Initializing parameters\n",
    "*Forward Proogation \n",
    "*Cost function\n",
    "*Back Propogation\n",
    "*Updation of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing kernels and creating helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2\n",
      "1      3\n",
      "2      2\n",
      "3      3\n",
      "4      3\n",
      "      ..\n",
      "413    3\n",
      "414    1\n",
      "415    3\n",
      "416    3\n",
      "417    1\n",
      "Name: Embarked, Length: 418, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\"train.csv\")\n",
    "data_train.head(0)\n",
    "data_train = data_train.dropna(subset = ['Embarked'])\n",
    "X_train = pd.DataFrame(data_train[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']])\n",
    "Y_train = pd.DataFrame(data_train[['Survived']])\n",
    "Y_train = Y_train.to_numpy()\n",
    "\n",
    "#print(X_train.dtypes)\n",
    "data_test = pd.read_csv(\"test.csv\")\n",
    "data_test.head()\n",
    "X_test_orig = pd.DataFrame(data_test[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gender = {\"male\": 1, \"female\": 2}\n",
    "X_train.Sex = [gender[item] for item in X_train.Sex]\n",
    "cab  = {'C': 1, 'Q': 2, 'S': 3}\n",
    "X_train.Embarked = [cab[item] for item in X_train.Embarked]\n",
    "\n",
    "\n",
    "Age_mean = X_train.mean(skipna = True)\n",
    "#print(Age_mean[\"Age\"])\n",
    "X_train['Age'] = X_train['Age'].fillna(Age_mean[\"Age\"])\n",
    "\n",
    "#Normalising the data\n",
    "X_train = (X_train - X_train.mean())/(X_train.std())\n",
    "\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_0 = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "X_train = np.insert(X_train, 0, 1, axis = 1)\n",
    "X_test_orig.Sex =  [gender[item] for item in X_test_orig.Sex]\n",
    "X_test_orig.Embarked = [cab[item] for item in X_test_orig.Embarked]\n",
    "X_test_orig['Age'] = X_test_orig['Age'].fillna(0)\n",
    "X_test = X_test_orig.to_numpy()\n",
    "X_test = np.insert(X_test, 0, 1, axis = 1)\n",
    "X_train1 = X_train[:700, :]\n",
    "X_train2 = X_train[700:, :]\n",
    "Y_train1 = Y_train[:700, :]\n",
    "Y_train2 = Y_train[700:, :]\n",
    "print(X_test_orig.Embarked)\n",
    "#print(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    To compute the sigmoid of z\n",
    "    z is an array\n",
    "    Returns s -> sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60613811 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.90406117 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.95887819 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.90406117 0.60613811 0.38344963 0.90406117 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.60613811 0.60613811\n",
      " 0.38344963 0.38344963 0.79202035 0.60613811 0.60613811 0.60613811\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.79202035 0.60613811 0.95887819 0.38344963 0.60613811 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.98296435\n",
      " 0.38344963 0.60613811 0.90406117 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.95887819 0.79202035 0.38344963 0.98296435 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.90406117 0.60613811 0.38344963 0.90406117 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.79202035 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.95887819 0.79202035\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.79202035 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.99885749 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.95887819 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.95887819 0.60613811 0.38344963 0.38344963\n",
      " 0.60613811 0.90406117 0.38344963 0.38344963 0.38344963 0.99885749\n",
      " 0.38344963 0.95887819 0.79202035 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.99885749 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.90406117 0.60613811 0.38344963 0.38344963 0.95887819 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.79202035 0.38344963\n",
      " 0.38344963 0.60613811 0.60613811 0.38344963 0.60613811 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.95887819 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.95887819 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.95887819 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.60613811 0.60613811 0.38344963 0.38344963 0.79202035 0.60613811\n",
      " 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811 0.99885749\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963 0.79202035\n",
      " 0.38344963 0.38344963 0.79202035 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.90406117 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.90406117 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.98296435 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.79202035 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963\n",
      " 0.90406117 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811\n",
      " 0.38344963 0.60613811 0.60613811 0.79202035 0.79202035 0.60613811\n",
      " 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.79202035 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963 0.98296435\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.90406117 0.60613811\n",
      " 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.95887819 0.95887819 0.60613811 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.60613811 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.60613811\n",
      " 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811 0.79202035\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.60613811 0.60613811 0.60613811 0.79202035\n",
      " 0.38344963 0.60613811 0.60613811 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.90406117 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963 0.90406117\n",
      " 0.38344963 0.79202035 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.38344963 0.60613811 0.38344963 0.38344963 0.79202035\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.79202035 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.98296435 0.60613811\n",
      " 0.60613811 0.95887819 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963 0.90406117\n",
      " 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.60613811 0.79202035 0.60613811 0.38344963\n",
      " 0.60613811 0.60613811 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.60613811 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.95887819 0.60613811 0.38344963 0.38344963 0.38344963 0.99885749\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963 0.38344963 0.95887819 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.90406117 0.60613811 0.38344963 0.38344963\n",
      " 0.38344963 0.95887819 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.60613811 0.60613811 0.38344963 0.38344963 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.99885749 0.38344963\n",
      " 0.38344963 0.60613811 0.95887819 0.38344963 0.60613811 0.38344963\n",
      " 0.60613811 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.79202035 0.60613811 0.38344963 0.99885749 0.38344963 0.38344963\n",
      " 0.60613811 0.38344963 0.38344963 0.60613811 0.38344963 0.60613811\n",
      " 0.38344963 0.38344963 0.60613811 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963 0.38344963\n",
      " 0.38344963 0.38344963 0.38344963 0.38344963 0.60613811 0.38344963\n",
      " 0.38344963]\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(X_train[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_paras(dims):\n",
    "    \"\"\"\n",
    "    To create the weights of dimension (n_h, 1) and a scalar b\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros(shape=(dims, 1))\n",
    "    b = 0\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] 0\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize_paras(X_train.shape[1])\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp( X, Y, w, b):\n",
    "    \"\"\"\n",
    "    To perform the forward propogation step\n",
    "    Returns a dictionary with the gradients and the cost separately\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    A = sigmoid(np.dot(X,w) + b)\n",
    "    cost = (-1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "        \n",
    "    dw = (1/m) * np.dot(X.T, (A-Y))\n",
    "    db = (1/m) * np.sum(A - Y)\n",
    "    \n",
    "    grads = {\"dw\":dw, \"db\":db}\n",
    "\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, c = forwardProp( X_train, Y_train, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDescent( X, Y, w, b, iterations, rate):\n",
    "    \"\"\"\n",
    "    Runs gradient decent to optimize w and b\n",
    "    Returns optimized parameters, accumulated gradients and all the \n",
    "    costs to see if it is decreasing\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        grads, cost = forwardProp(X, Y, w, b)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        #Updating weights\n",
    "        w = w - dw*rate\n",
    "        b = b - db*rate\n",
    "        \n",
    "        #Appending cost to the list\n",
    "        if i%100 ==0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    \n",
    "    #Creating dictionaries to store the optimized parameters\n",
    "    paras = {\"w\":w, \"b\":b}\n",
    "    grads = {\"dw\":dw, \"db\":db}\n",
    "    \n",
    "    return paras, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'w': array([[-0.32322355],\n",
       "         [-0.86564023],\n",
       "         [ 1.29298582],\n",
       "         [-0.49311783],\n",
       "         [-0.35264481],\n",
       "         [-0.08610664],\n",
       "         [ 0.12460108],\n",
       "         [-0.17452079]]),\n",
       "  'b': -0.3232235483739071},\n",
       " {'dw': array([[ 2.24417920e-04],\n",
       "         [ 1.99241936e-03],\n",
       "         [-7.80068741e-04],\n",
       "         [ 1.20074580e-03],\n",
       "         [ 6.31806819e-04],\n",
       "         [-2.07953466e-04],\n",
       "         [ 1.35698100e-03],\n",
       "         [ 4.82237793e-05]]),\n",
       "  'db': 0.0002244179199354488},\n",
       " [0.6931471805599454])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientDescent(X_train, Y_train, w, b, 100, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    To predict the Y value for a corresponding X value given.\n",
    "    Returns the prediction for all x in X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]# No. of training examples\n",
    "    Predictions = np.zeros((m, 1))\n",
    "    \n",
    "    z = np.dot(X, w) + b\n",
    "    A = sigmoid(z)\n",
    "    \n",
    "    for i in range(m):\n",
    "        if A[i, 0] > 0.5:\n",
    "            Predictions[i, 0] = 1\n",
    "        else:\n",
    "            Predictions[i, 0] = 0\n",
    "            \n",
    "    return Predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, iterations=2000, rate=0.9):\n",
    "    \"\"\"\n",
    "    Puts together functions created as a single model\n",
    "    Returns details about the model\n",
    "    \"\"\"\n",
    "    print(\"Running\")\n",
    "    dims = X_train.shape[1]\n",
    "    w, b = initialize_paras(dims)\n",
    "    \n",
    "    paras, grads, costs = GradientDescent(X_train, Y_train, w, b, iterations, rate)\n",
    "    w = paras[\"w\"]\n",
    "    b = paras[\"b\"]\n",
    "    \n",
    "    Prediction_test = predict(w, b, X_test)\n",
    "    Prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    #Calculating accuracies\n",
    "    Train_accuracy = 100 - np.mean(np.abs(Prediction_train - Y_train)) * 100\n",
    "    print(\"Training set accuracy: \" + str(Train_accuracy))\n",
    "    \n",
    "    details = {\"costs\":costs, \"w\":w, \"b\":b, \"rate\":rate, \"iterations\":iterations, \n",
    "              \"Prediction_test\":Prediction_test, \"Prediction_train\":Prediction_train}\n",
    "    \n",
    "    return details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n",
      "Training set accuracy: 79.71428571428572\n",
      "63.492063492063494\n",
      "[0.6931471805599454, 0.45071734403346375, 0.45071604132029286, 0.4507160411483511, 0.4507160411483282, 0.4507160411483282, 0.4507160411483282, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814, 0.45071604114832814]\n"
     ]
    }
   ],
   "source": [
    "details = model(X_train1, Y_train1, X_test)\n",
    "p2 = predict(w, b, X_train2)\n",
    "Train_accuracy1 = 100 - np.mean(np.abs(p2 - Y_train2)) * 100\n",
    "print(Train_accuracy1)\n",
    "print(details[\"costs\"])\n",
    "#print(details[\"Prediction_train\"] - Y_train)\n",
    "P = details[\"Prediction_test\"].T\n",
    "P = np.squeeze(np.array(P))\n",
    "#print(P)\n",
    "submission = pd.DataFrame({'PassengerId': data_test['PassengerId'], 'Survived':P.T})\n",
    "#submission.to_csv('Titanic Predictions LR1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
